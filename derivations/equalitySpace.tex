\documentclass[]{article}

%opening
\title{The Model Space of Equality Constraints}
\author{}
\date{}

\usepackage{amsmath,amsfonts,amssymb, bm}

\usepackage{caption, subcaption}
\usepackage{booktabs}
\usepackage{float}
\usepackage{nicefrac}
\usepackage[colorlinks=true, allcolors=blue, bookmarks=false]{hyperref}

\usepackage{todonotes}
\newcommand{\DON}[1]{\todo[inline, color = white]{Don: #1}}


\usepackage[style=apa,sortcites=true,sorting=nyt,backend=biber]{biblatex}
\addbibresource{../references.bib}

    
\newcommand{\bindicator}{\gamma}% binary indicator
\newcommand{\cindicator}{\kappa}% categorical indicator


\newcommand{\stirling}[2]{\genfrac{\{}{\}}{0pt}{}{#1}{#2}} % https://tex.stackexchange.com/a/228111/115231

\newcommand{\bellnum}[2]{B\left(#1, #2\right)}

\newcommand{\density}[1]{\pi\left(#1\right)}
\newcommand{\categorical}[1]{\text{Categorical}\left(#1\right)}
\newcommand{\frequency}[1]{\text{Freq}\left(#1\right)}

\begin{document}

\maketitle

\section{Outline}

\begin{itemize}
    \item Motivation: Multiple comparison, more elegant to look at all partitions (question of probability spreading out); but could also implement only pairwise comparisons (compare the two to show it is more elegant; explore paradoxical situations)
    \item Compare to Dirichlet Process Prior \parencite{gopalan1998bayesian}
    \item Section on the beta-binomial prior (literature on shrinkage)
    \item Apply it to ANOVA
\end{itemize}

\section{Todo}
\begin{itemize}
    \item Don creates Scott and Berger plots
    \item Don drafts methods section
    \item Fabian drafts introduction
    \item Fabian fixes Gopalan and Berry implementation (in R or Julia)
    \item Don fixes ANOVA implementation \checkmark
    \item Don fixes beta-binomial prior in Turing \checkmark
    \item Fabian puts bfvartest on CRAN
    \item Don and Fabian read Gibbs sampler \textcite{gopalan1998bayesian} \checkmark
    \item Don implements the uniform prior \checkmark
    \item Fabian looks into relationship between Dirichlet Process prior and beta-binomial prior \checkmark
    \begin{itemize}
        \item Figured out how to count the number of equalities, see bottom of this document
        \item One interesting thing: the beta-binomial treats the models $(\mu_1 = \mu_2, \mu_3 = \mu_4)$ as the same as $(\mu_1 = \mu_2 = \mu_3, \mu_4$), while the Dirichlet process prior puts more prior mass on the latter model.
    \end{itemize}
    \item Don adjusts prior so that it is uniform over all equalities. Also, how does the induced prior on the number of equalities look like when the prior on the models is uniform? \checkmark
    \item Fabian reads up on subsequent work that cites \textcite{gopalan1998bayesian} \checkmark
\end{itemize}

\section{Comments}
\begin{itemize}
    \item If we want to use regression, then the predictors are likely correlated, which would probably merit a different approach compared to, say, ANOVA, where we randomly assign groups
    \item It would be good to think about how would one go about doing multiple pairwise tests without our method, and whether there are any paradoxes lurking (for example, one can reject the full model but find no evidence of any pairwise differences --- this could be a another selling point of our method)
    \item A uniform prior over the model space does not lead to multiplicity control \parencite{de2019bayesian, scott2010bayes}.
    \item So \textcite{de2019bayesian} looked at multiplicity control in his thesis. From a glance, it seems that he implemented a null control method (ala Jeffreys and West, similar to Bonferroni), but failed to apply the idea from \textcite{scott2006exploration} to the case of ANOVA. So what is implemented in JASP right now?
    \item Tim is only looking at pairwise comparisons, probably because these can be efficiently computed, that is, there is no explosion of possibilities that we have by looking at all partitions. However, for small number of groups, say $k = 5$ or $k = 6$, the Bell numbers are 15 and 52, respectively, and so one could compute the marginal likelihood of all partitions. Using a particular model prior, one would then adjust for multiplicity. So maybe our use case is not particularly strong, actually. It seems that we're solving the $20 > k > 6$ problem. Is that so groundbreaking? \textcite{kang2017objective} seem to tackle the small $k$ case for multiple comparisons on variances using a fractional Bayes factor
\end{itemize}

\section{Literature}
\begin{itemize}
    \item \textcite{rao2009multiple} provides an extensive bibliography of the multiple comparison literature; distinguishes between MCA (all-pairwise comparisons), MCB (multiple comparisons with the best), MCC (multiple comparisons with a control), and MCM (multiple comparisons with the mean)
    \item \textcite{gutierrez2019bayesian} implement a non-parametric test for the MCC case
    \item \textcite{gopalan1998bayesian} study the \textit{exact} same problem we have, in the context of the comparison of population means. They use a Dirichlet Process prior and Gibbs sampling. We need to read this paper!
    \begin{itemize}
        \item I think this is very elegant. However, the prior is fully determined by two parameters: the number of groups $k$ and a concentration parameter $M$. As $M \rightarrow 0$, we have that $P(\mathcal{H}_0) \rightarrow 1$, and that as $M \rightarrow \infty$, we have that $P(\mathcal{H}_N) \rightarrow 1$. Similarly, all partitions have equal prior probability (e.g., $\mu_1 = \mu_2$ is equally likely as $\mu_3 = \mu_4$) --- but we have that as well currently.
        \item Their method is also quite straightforward, and a Gibbs sampler is easily developed. The probability that $\mu_i$ is equal to any of the previous means is given by $\nicefrac{M}{M + k - 1}$.
        \item How is our approach related to theirs? It seems we must be able to transform our prior into theirs, and then get equivalent results. I think it makes sense to study their prior for various combinations of $k$ and $M$ and to see how our beta-binomial prior matches this. It seems that ours is more flexible? We should explore this and see if our approach is still valuable. What about computational feasibility? Maybe it is better to find some generalization of the Dirichlet Process prior that is more flexible, but can be efficiently implemented? For example the \href{https://en.wikipedia.org/wiki/Pitman\%E2\%80\%93Yor\_process}{Pitman-Yor process}.
    \end{itemize}
    \item \textcite{berry1999bayesian} provide an overview of Bayesian methods for adjusting for multiplicity, also discussing \textcite{gopalan1998bayesian}. Seems important for context, and it looks like their paper is more accessible than \textcite{gopalan1998bayesian}.
    \item \textcite{kim2009spiked} use a Dirichlet Process prior for random effects models.
    \item \textcite{curtis2011bayesian} also use a Dirichlet Process prior to cluster highly correlated predictors in linear regression. (``In our model highly correlated predictors can be grouped together by setting their corresponding coefficients exactly equal.'')
    \item \textcite{dunson2008bayesian} use a Dirichlet process prior for clustering in the context of genetics.
    \item \textcite{castillo2020spike} show that spike-and-slab variable selection has good frequentist multiple testing properties (they only study inclusion, not pairwise equality; however, inclusion in the case of ANOVA under dummy coding would translate into non-equivalence of groups; let's think about various codings)
    \item \textcite{canale2017pitman} discuss the Pitman-Yor process instead of the Dirichlet Process
\end{itemize}


Note, this is written chronologically from how I went from A to B. This is probably not a good way to present this to a journal.
I'm not entirely sure whether we should frame the problem as a graph or as the counting of partitions.
Viewing it as counting of partitions opens up much more relations to combinatorics, but I understand that perspective less well than that of a graphs.

\subsection*{Problem Description}
Given $K$ parameters, $\theta_1,\, \dots,\, \theta_K$, we are interested in exploring all possible equality constraints.
This problem can be viewed as an undirected graph where the parameters are vertices and two parameters are equal if and only if there is an edge between them.
For example for $K = 3$ we have:
\begin{table}[!ht]
	\centering
	\begin{tabular}{l|rrr}
					&	$\theta_1$	&	$\theta_2$	&	$\theta_3$	\\
		\hline
		$\theta_1$	&				&				&				\\
		$\theta_2$	&	$\bindicator_1$	&				&				\\
		$\theta_3$	&	$\bindicator_2$	&	$\bindicator_3$	&				\\
	\end{tabular}
\end{table}

where $\bindicator_i \in \{0, 1\} \text{ for } i=1, 2, 3$.
However, if we list all possible models, it becomes clear that this parametrization gives rise to duplicate models.
For example, $\bm{\bindicator} = (1, 1, 1), \bm{\bindicator} = (1, 1, 0), \bm{\bindicator} = (1, 0, 1), \text{ and } \bm{\bindicator} = (0, 1, 1)$ all represent the same model where all parameters are equal.
At the same time, no single $\bindicator_i$ is redundant but only certain combinations of the edges are.
There are multiple ways to constrain the adjacency matrix such that duplicate models are impossible and we discuss one of these here.
The first constraint that we impose is that the rows of the adjacency matrix sum to one.
This makes the combination $\bm{\bindicator} = (0, 1, 1)$ impossible. 
The second constraint asserts that there can only be one edge to a connected set of vertices, and that a connection can only be made with the `first' vertex of a connected set.
For example, given $\bindicator_1 = 1$ it follows that $\bindicator_3 = 0$.

To satisfy the first constraint, we reparametrize the binary indicator variables to categorical ones, that is, $\cindicator_1 = 1, \cindicator_2 \in \{1, 2\}, \cindicator_3 \in \{1, 2, 3\} $.\footnote{Note that although $\cindicator_1$ is redundant we use it to ease the notation.}
Two parameters are equal whenever their indicator values are equal, for example, $\bm{\cindicator} = (1, 2, 2)$ implies $\theta_2 = \theta_3$. 
The second constraint can be formulated as $\cindicator_2 \neq 2 \implies \cindicator_3 \neq 2$. 
More generally, this means that $\cindicator_i \neq i \implies \cindicator_j \neq i $ for $j = i + 1, \,\dots,\,K$.

For illustrative purposes we list all possible model for $K=3$ under both parametrizations and whether they violate any restrictions.
\begin{table}[H]
	\centering
	\caption{All possible configurations for both sets of indicator variables and the whether the corresponding models violate any constraints.}
	\label{tb:modelsk3}
	\captionsetup[subtable]{position = top}
	\captionsetup[table]{position=top}
	\begin{subtable}{0.3\linewidth}
		\centering
		\caption*{Binary indicator}
		\begin{tabular}{rrr}
			\toprule
			$\bm{\bindicator}$ 		& valid\\
			\midrule
			$0,0,0$					& $\checkmark$\\
			$0,0,1$					& $\checkmark$\\
			$0,1,0$					& $\checkmark$\\
			$1,0,0$					& $\checkmark$\\
			$0,1,1$					& violates constraint 1\\
			$1,0,1$					& violates constraint 2\\
			$1,1,0$					& $\checkmark$\\
			$1,1,1$					& violates constraint 1\\
			\bottomrule
		\end{tabular}
	\end{subtable}
	\hspace*{4em}
	\begin{subtable}{0.3\linewidth}
		\centering
		\caption*{Categorical indicator}
		\begin{tabular}{rr}
			\toprule
			$\bm{\cindicator}$	 	& valid\\
			\midrule
			$1, 1, 1$				& $\checkmark$\\
			$1, 1, 2$				& violates constraint 2\\
			$1, 1, 1$				& $\checkmark$\\
			$1, 2, 1$				& $\checkmark$\\
			$1, 2, 2$				& $\checkmark$\\
			$1, 2, 3$				& $\checkmark$\\
			\bottomrule
		\end{tabular}
	\end{subtable}
\end{table}

From the table above we see there are in total 5 valid models.
Counting the number of possible models is equivalent to counting the number of partitions into non-empty subsets of a set of size $K$.
Therefore, given $K$ parameters the number of possible models is given by the $K^\text{th}$ Bell number, $B_K$.
The model space grows exponentially in $K$, for example, $B_3 = 5,\, B_5 = 52,\, B_{10} = 115 \,975$.


\subsection*{Sampling Models Uniformly}
Here we construct a Gibbs sampler to sample uniformly from the model space.
A naive approach is to sample from a uniform categorical distribution while satisfying constraint 2.
However, this distribution is not uniform over the models space.
From Table~\ref{tb:modelsk3} we can see that there are 5 valid models, and that there are 2 models where $\kappa_2 = 1$ and 3 models where $\kappa_2 = 2$.
Adjusting the probabilities boils down to counting how often a particular outcome occurs conditional on the previous indicator variables.
These counts can be obtained using the $r\text{-Bell numbers}$, which are defined as \parencite{mezo2011r}:
\begin{align*}
	\bellnum{n}{r} = 
	\sum_{k=0}^{n} \stirling{n+r}{k+r}_r =
	\sum_{k=0}^{n} \left(\sum_{i=0}^{n} \binom{n}{i} \stirling{i}{k} r^{n - i} \right)
\end{align*}
where $\stirling{n+r}{k+r}_r$ denotes the $r\text{-Stirling}$ number and $\stirling{i}{k}$ denotes the Stirling partition number.
The left definition can be interpreted as the number of the partitions of a set with n + r element such that the first r elements are in distinct subsets in each partition.
Let $s_i$ denote the number of active equality constraints, e.g., $s_i = \sum_{j = 1}^{i-1} I(\cindicator_j = j)$, where $I(\cdot)$ is an indicator function that returns 1 if its argument is true and 0 otherwise.
Then, the frequency of each possible outcome for $\cindicator_i$ is given by
\begin{align*}
	\frequency{\cindicator_i = j} = \begin{cases}
	\bellnum{K - i - 1}{s_i + 1} 		& \text{if } i = j					\\
	\bellnum{K - i - 1}{s_i} 			& \text{if } \cindicator_j = j 		\\
	0 									& \text{if } \cindicator_j \neq j
	\end{cases}
\end{align*}
For example, for $K = 3$ we have $\frequency{\cindicator_2 = 1} = \bellnum{1}{1} = 2$ and $\frequency{\cindicator_2 = 2} = \bellnum{1}{2} = 3$, which matches Table~\ref{tb:modelsk3}.

Using the frequencies we obtain the following scheme to sample uniformly from the model space:
\begin{align*}
\density{\cindicator_1} &= \categorical{1}\\
\density{\cindicator_2 \mid \cindicator_1} 				  &= \categorical{\bellnum{K-2}{1},\,\bellnum{K-2}{2}}\\
\density{\cindicator_3 \mid \cindicator_1, \cindicator_2} &= \categorical{\frequency{\cindicator_3 = 1}, \frequency{\cindicator_3 = 2}, \frequency{\cindicator_3 = 3}} \\
&\vdots\\
\density{\cindicator_K \mid \cindicator_1, \dots, \cindicator_{K-1}} &= \categorical{\frequency{\cindicator_K = 1}, \dots, \frequency{\cindicator_K = K}} 
\end{align*}
Here it is assumed that the probability vectors for the categorical distributions are normalized so they sum to 1.

\DON{
A possible problem here is that the sampler may have difficulties to transition between models that are intuitively close together.
For example, given $K = 3$ and $\bm{\cindicator} = (1, 2, 2)$ (one equality constraint) it is not allowed to go to $(2, 2, 2)$  (two equality constraints).
Instead we'd need to do $(1, 2, 3) \rightarrow (1, 1, 3) \rightarrow (1, 1, 1)$. 
Note that $(1, 1, 2)$ is also an impossible state.
A solution for this is to let $\cindicator_i \in \{1, \dots, K\} \forall i$.
However, that introduces a large number of duplicate models we'd need to adjust for.
So far I've not encountered any issues with this yet, but maybe we will when $K$ grows larger.
}

\subsection*{A Beta-Binomial Penalty}
As in Linear regression, a uniform distribution over models is not uniform over the number of equality constraints.
To achieve this, we implement a beta-binomial penalty.
First, we need to count the number of models with $k$ number of equality constraints for $k=1,\,\dots,\,K$.

\subsection*{Dirichlet Process Prior}
For the DPP, not only the number of equalities but also their position matters. For example, while the beta-binomial treats the models $(\mu_1 = \mu_2, \mu_3 = \mu_4)$ as the same as $(\mu_1 = \mu_2 = \mu_3, \mu_4$), the Dirichlet process prior puts more prior mass on the latter model.

Following \textcite{gopalan1998bayesian}, we write $(\mu_1, \ldots, \mu_k) \in C(m_1, \ldots, m_k)$ if there are $m_1$ distinct values of $\mu$ that occur exactly once, $m_2$ distinct values of $\mu$ that occur exactly twice, $\ldots$, and $m_k$ distinct values of $\mu$ that occur exactly $k$ times. The number of models with $k$ number of equality constraints is then given by:

\begin{equation}
    \prod_{j = 1}^K \frac{1}{C[j]}{K - j + 1 \choose C[j]} \enspace .
\end{equation}

The reasoning is as follows. Suppose that we have 2 equality constraints in a group of five, which could look like this $(\mu_1 = \mu_2, \mu_3 = \mu_4, \mu_5) \in C(1, 2, 0, 0, 0)$. How many models such as this are there? There are ${5 \choose 1} = 5$ possible parameters that could be different from the rest. Once this parameter is fixed, we need to count possibilities for the four others; hence we continue as if we only had 4 parameters left. There are ${4 \choose 2} = 6$ ways in which two parameters can be equal, but this overcounts by a factor of two since $(\mu_1 = \mu_2, \mu_3 = \mu_4)$ is the same as $(\mu_3 = \mu_4, \mu_1 = \mu_2$). Hence we have ${5 \choose 1} \times {4 \choose 2} \times \frac{1}{2} = 15$ models that have 2 equality constraints in five groups.



\printbibliography

\end{document}